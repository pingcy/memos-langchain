"""
ğŸ¤– Chatbot with MemOS - åŸºäº LangChain çš„é•¿æœŸè®°å¿†èŠå¤©æœºå™¨äºº
============================================================

åŠŸèƒ½æ¼”ç¤º:
1. ä½¿ç”¨ LangChain ChatOpenAI ä½œä¸ºå¯¹è¯æ¨¡å‹
2. ä½¿ç”¨ MemOS GeneralTextMemory å®ç°é•¿æœŸè®°å¿†
3. å¢é‡å¼è®°å¿†ç®¡ç†ï¼ˆåªæ·»åŠ æ–°çš„å¯¹è¯ï¼Œä¸é‡å¤æ·»åŠ å†å²ï¼‰
4. åŸºäºè®°å¿†çš„ä¸ªæ€§åŒ–å¯¹è¯

ä¾èµ–:
- pip install langchain langchain-openai
- Qdrant (æœ¬åœ°æ–‡ä»¶å­˜å‚¨ï¼Œæ— éœ€é¢å¤–æœåŠ¡)
- OpenAI API
"""

import warnings
import logging
import os
from datetime import datetime
from typing import List, Dict, Optional

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", message="Pydantic serializer warnings")
warnings.filterwarnings("ignore", message="`torch_dtype` is deprecated")
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message=".*Neo4j.*")
warnings.filterwarnings("ignore", message=".*relationship type.*")
warnings.filterwarnings("ignore", message=".*PARENT.*")

# è®¾ç½®æ—¥å¿—çº§åˆ« - æŠ‘åˆ¶ memos å†…éƒ¨çš„è°ƒè¯•è¾“å‡º
logging.basicConfig(level=logging.ERROR, format='%(message)s')  # åªæ˜¾ç¤º ERROR ä»¥ä¸Š
logging.getLogger("memos").setLevel(logging.ERROR)
logging.getLogger("memos.api.config").setLevel(logging.CRITICAL)
logging.getLogger("memos.mem_cube").setLevel(logging.CRITICAL)  # æŠ‘åˆ¶ mem_cube è­¦å‘Š
logging.getLogger("memos.mem_cube.general").setLevel(logging.CRITICAL)  # æŠ‘åˆ¶ pref_mem è­¦å‘Š
logging.getLogger("neo4j").setLevel(logging.CRITICAL)  # å®Œå…¨æŠ‘åˆ¶ Neo4j æ—¥å¿—
logging.getLogger("neo4j.notifications").setLevel(logging.CRITICAL)
logging.getLogger("neo4j.io").setLevel(logging.CRITICAL)
logging.getLogger("httpx").setLevel(logging.ERROR)

# æŠ‘åˆ¶ memos å†…éƒ¨çš„ trace-id æ ¼å¼è­¦å‘Šï¼ˆéœ€è¦åœ¨ import memos ä¹‹å‰è®¾ç½®ï¼‰
import memos.settings
memos.settings.DEBUG = False
# è®¾ç½® memos çš„æ ¹æ—¥å¿—çº§åˆ«ä¸º ERROR
logging.getLogger("memos").setLevel(logging.ERROR)

from dotenv import load_dotenv
load_dotenv()

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# MemOS imports
from memos.configs.mem_os import MemOSConfigFactory
from memos.mem_os.main import MOS
from memos.mem_cube.general import GeneralMemCube
from memos.configs.mem_cube import GeneralMemCubeConfig

# å†æ¬¡è®¾ç½®æ—¥å¿—çº§åˆ«ï¼ˆç¡®ä¿åœ¨ memos æ¨¡å—å¯¼å…¥åç”Ÿæ•ˆï¼‰
logging.getLogger("memos").setLevel(logging.ERROR)
logging.getLogger("memos.mem_cube.general").setLevel(logging.ERROR)
logging.getLogger("memos.api.config").setLevel(logging.ERROR)


# ==================== è®°å¿†ç³»ç»Ÿ Prompt æ¨¡æ¿ ====================
MEMORY_SYSTEM_PROMPT = """# Role
ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰é•¿æœŸè®°å¿†èƒ½åŠ›çš„æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ (Travel Assistant)ã€‚
ä½ çš„ç›®æ ‡æ˜¯ç»“åˆæ£€ç´¢åˆ°çš„è®°å¿†ç‰‡æ®µï¼Œä¸ºç”¨æˆ·æä¾›é«˜åº¦ä¸ªæ€§åŒ–ä¸”é€»è¾‘ä¸¥å¯†çš„å›ç­”ã€‚
ä½ åº”è¯¥åœ¨æ¯æ¬¡ç»™å‡ºå»ºè®®å‰å°½é‡äº†è§£ç”¨æˆ·çš„ä¿¡æ¯ã€åå¥½å¹¶åŠæ—¶è°ƒæ•´ã€‚
å°½é‡ç®€çŸ­çš„å›ç­”é—®é¢˜ã€‚

# System Context
- å½“å‰æ—¶é—´: {current_time}

# Memory Data
ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„ä¸ç”¨æˆ·ç›¸å…³çš„è®°å¿†ä¿¡æ¯ï¼š

<memories>
{memories}
</memories>

# Instructions
1. ç»“åˆè®°å¿†ä¸­çš„ä¿¡æ¯ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–å›ç­”
2. å¦‚æœè®°å¿†ä¸å½“å‰é—®é¢˜æ— å…³ï¼Œå¯ä»¥å¿½ç•¥
3. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦æåŠ"è®°å¿†"ã€"æ£€ç´¢"ç­‰ç³»ç»Ÿå†…éƒ¨æœ¯è¯­
4. å¦‚æœè®°å¿†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ­£å¸¸å›ç­”å³å¯"""


class MemOSChatbot:
    """åŸºäº LangChain å’Œ MemOS çš„é•¿æœŸè®°å¿†èŠå¤©æœºå™¨äºº
    
    ä½¿ç”¨ tree_text (Neo4j) åç«¯ï¼Œè®°å¿†æ•°æ®ç›´æ¥å­˜å‚¨åœ¨ Neo4j æ•°æ®åº“ä¸­ï¼Œ
    æ— éœ€æ‰‹åŠ¨ dump/loadã€‚
    """
    
    def __init__(
        self,
        user_id: str = "chatbot_user",
        model_name: str = "gpt-4o-mini",
        temperature: float = 0.7,
        top_k: int = 5
    ):
        """
        åˆå§‹åŒ– Chatbot
        
        Args:
            user_id: ç”¨æˆ·IDï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„è®°å¿†
            model_name: OpenAI æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            top_k: è®°å¿†æ£€ç´¢æ•°é‡
        """
        self.user_id = user_id
        self.top_k = top_k
        
        # è·å– API é…ç½®
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        
        if not self.openai_key:
            raise ValueError("âŒ æœªé…ç½® OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        
        # åˆå§‹åŒ– LangChain ChatOpenAI
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            openai_api_key=self.openai_key,
            openai_api_base=self.openai_base
        )
        
        # åˆå§‹åŒ– MemOSï¼ˆä¼šå°è¯•åŠ è½½å·²æœ‰è®°å¿†ï¼‰
        self.is_new_user = self._init_memos()
        
        # å½“å‰ä¼šè¯çš„å¯¹è¯å†å²ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        self.conversation_history: List[Dict[str, str]] = []
        
        # å·²ç»æ·»åŠ åˆ°è®°å¿†åº“çš„æ¶ˆæ¯æ•°ï¼ˆç”¨äºå¢é‡æ·»åŠ ï¼‰
        self.memorized_message_count = 0
        
        # ä¼šè¯IDï¼ˆç”¨äºè¿½è¸ªï¼‰
        self.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"âœ… Chatbot åˆå§‹åŒ–å®Œæˆ")
        print(f"   - ç”¨æˆ·ID: {self.user_id}")
        print(f"   - ä¼šè¯ID: {self.session_id}")
        print(f"   - æ¨¡å‹: {model_name}")
        print(f"   - æ˜¯å¦æ–°ç”¨æˆ·: {'æ˜¯' if self.is_new_user else 'å¦ï¼ˆå·²æœ‰å†å²è®°å¿†ï¼‰'}")
    
    def _init_memos(self) -> bool:
        """
        åˆå§‹åŒ– MemOS è®°å¿†ç³»ç»Ÿ
        
        å¯¹äº tree_text (Neo4j) åç«¯ï¼Œæ•°æ®ç›´æ¥å­˜åœ¨æ•°æ®åº“ä¸­ï¼Œä¸éœ€è¦ dump/loadã€‚
        åªéœ€è¦æŸ¥è¯¢ Neo4j ä¸­æ˜¯å¦æœ‰è¯¥ç”¨æˆ·çš„è®°å¿†æ¥åˆ¤æ–­æ˜¯å¦æ–°ç”¨æˆ·ã€‚
        
        Returns:
            bool: True è¡¨ç¤ºæ˜¯æ–°ç”¨æˆ·ï¼ˆæ— å†å²è®°å¿†ï¼‰ï¼ŒFalse è¡¨ç¤ºå·²æœ‰å†å²è®°å¿†
        """
        print("ğŸ“¦ åˆå§‹åŒ– MemOS è®°å¿†ç³»ç»Ÿ...")
        
        # é…ç½® MOS
        mos_config = MemOSConfigFactory(
            config={
                "user_id": "root",
                "chat_model": {
                    "backend": "openai",
                    "config": {
                        "model_name_or_path": "gpt-4o-mini",
                        "temperature": 0.0,
                        "max_tokens": 8192,
                        "api_key": self.openai_key,
                        "api_base": self.openai_base
                    }
                },
                "mem_reader": {
                    "backend": "simple_struct",
                    "config": {
                        "llm": {
                            "backend": "openai",
                            "config": {
                                "model_name_or_path": "gpt-4o-mini",
                                "temperature": 0.0,
                                "max_tokens": 8192,
                                "api_key": self.openai_key,
                                "api_base": self.openai_base
                            }
                        },
                        "embedder": {
                            "backend": "universal_api",
                            "config": {
                                "provider": "openai",
                                "model_name_or_path": "text-embedding-3-small",
                                "api_key": self.openai_key,
                                "base_url": self.openai_base
                            }
                        },
                        "chunker": {
                            "backend": "sentence",
                            "config": {
                                "tokenizer_or_token_counter": "character",
                                "chunk_size": 512,
                                "chunk_overlap": 128,
                                "min_sentences_per_chunk": 1
                            }
                        }
                    }
                },
                "max_turns_window": 20,
                "top_k": self.top_k,
                "enable_textual_memory": True,
                "enable_activation_memory": False,
                "enable_parametric_memory": False,
                "enable_mem_scheduler": False
            }
        )
        
        self.mos = MOS(mos_config.config)
        self.mos.create_user(user_id=self.user_id)
        
        # cube_id å›ºå®š
        self.cube_id = f"{self.user_id}_chatbot_cube"
        
        # Neo4j é…ç½®
        neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        neo4j_password = os.getenv("NEO4J_PASSWORD", "yourpassword")
        
        print(f"   ğŸ“¡ è¿æ¥ Neo4j: {neo4j_uri}")
        
        # åˆ›å»º MemCube è¿æ¥åˆ° Neo4j (tree_text åç«¯)
        mem_cube_config = GeneralMemCubeConfig(
            user_id=self.user_id,
            cube_id=self.cube_id,
            text_mem={
                "backend": "tree_text",
                "config": {
                    "extractor_llm": {
                        "backend": "openai",
                        "config": {
                            "model_name_or_path": "gpt-4o-mini",
                            "temperature": 0.0,
                            "max_tokens": 8192,
                            "api_key": self.openai_key,
                            "api_base": self.openai_base
                        }
                    },
                    "dispatcher_llm": {
                        "backend": "openai",
                        "config": {
                            "model_name_or_path": "gpt-4o-mini",
                            "temperature": 0.0,
                            "max_tokens": 8192,
                            "api_key": self.openai_key,
                            "api_base": self.openai_base
                        }
                    },
                    "embedder": {
                        "backend": "universal_api",
                        "config": {
                            "provider": "openai",
                            "model_name_or_path": "text-embedding-3-small",
                            "api_key": self.openai_key,
                            "base_url": self.openai_base
                        }
                    },
                    "graph_db": {
                        "backend": "neo4j",
                        "config": {
                            "uri": neo4j_uri,
                            "user": neo4j_user,
                            "password": neo4j_password,
                            "db_name": "memos",
                            "embedding_dimension": 1536
                        }
                    },
                    "reorganize": True
                }
            },
            act_mem={"backend": "uninitialized"},
            para_mem={"backend": "uninitialized"}
        )
        
        self.mem_cube = GeneralMemCube(mem_cube_config)
        self.mos.register_mem_cube(
            mem_cube_name_or_path=self.mem_cube,
            mem_cube_id=self.cube_id,
            user_id=self.user_id
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†å²è®°å¿†
        is_new_user = True
        try:
            # å°è¯•è·å–è®°å¿†æ•°é‡
            all_memories = self.mem_cube.text_mem.get_all(user_name=self.user_id)
            if isinstance(all_memories, dict) and 'nodes' in all_memories:
                memory_count = len(all_memories.get('nodes', []))
            else:
                memory_count = len(all_memories)
            
            if memory_count > 0:
                print(f"   âœ… å‘ç° {memory_count} æ¡å†å²è®°å¿†")
                is_new_user = False
            else:
                print(f"   ğŸ†• æœªå‘ç°å†å²è®°å¿†")
        except:
            print(f"   ğŸ†• æœªå‘ç°å†å²è®°å¿†")
        
        return is_new_user
    
    def _search_memories(self, query: str) -> List[str]:
        """
        æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³è®°å¿†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            è®°å¿†åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        """
        memories = []
        
        try:
            results = self.mos.search(query=query, user_id=self.user_id)
            
            if results.get("text_mem") and results["text_mem"][0]["memories"]:
                for mem_item in results["text_mem"][0]["memories"][:self.top_k]:
                    memories.append(mem_item.memory)
                    
        except Exception as e:
            print(f"   âš ï¸ æ£€ç´¢è®°å¿†æ—¶å‡ºé”™: {e}")
        
        return memories
    
    def _format_memories(self, memories: List[str]) -> str:
        """å°†è®°å¿†åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²"""
        if not memories:
            return "æš‚æ— ç›¸å…³è®°å¿†"
        
        formatted = []
        for i, mem in enumerate(memories, 1):
            formatted.append(f"[{i}] {mem}")
        
        return "\n".join(formatted)
    
    def _add_memories_incrementally(self):
        """å¢é‡å¼æ·»åŠ è®°å¿†åˆ° MemOS"""
        current_count = len(self.conversation_history)
        
        if current_count <= self.memorized_message_count:
            return
        
        new_messages = self.conversation_history[self.memorized_message_count:]
        
        if len(new_messages) >= 2:
            try:
                self.mos.add(
                    messages=new_messages,
                    user_id=self.user_id,
                    mem_cube_id=self.cube_id
                )
                self.memorized_message_count = current_count
                print(f"   ğŸ’¾ å·²å°† {len(new_messages)} æ¡æ–°æ¶ˆæ¯æ·»åŠ åˆ°è®°å¿†")
            except Exception as e:
                print(f"   âš ï¸ æ·»åŠ è®°å¿†æ—¶å‡ºé”™: {e}")
    
    def chat(self, user_input: str) -> str:
        """
        ä¸ Chatbot å¯¹è¯
        
        æµç¨‹ï¼š
        1. æ£€ç´¢ç›¸å…³è®°å¿†
        2. æ„å»ºå¸¦è®°å¿†ä¸Šä¸‹æ–‡çš„ prompt
        3. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
        4. å¢é‡æ·»åŠ å¯¹è¯åˆ°è®°å¿†
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            åŠ©æ‰‹å›å¤
        """
        # 1. æ£€ç´¢ç›¸å…³è®°å¿†
        memories = self._search_memories(user_input)
        formatted_memories = self._format_memories(memories)
        
        if memories:
            print(f"   ğŸ” æ£€ç´¢åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")
        
        # 2. æ„å»º prompt
        system_prompt = MEMORY_SYSTEM_PROMPT.format(
            current_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            memories=formatted_memories
        )
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«æœ€è¿‘å‡ è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
        messages = [SystemMessage(content=system_prompt)]
        
        # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡
        recent_history = self.conversation_history[-10:]  # æœ€è¿‘ 5 è½®
        for msg in recent_history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append(HumanMessage(content=user_input))
        
        # 3. è°ƒç”¨ LLM
        response = self.llm.invoke(messages)
        assistant_reply = response.content
        
        # 4. æ·»åŠ åˆ°å¯¹è¯å†å²
        self.conversation_history.append({"role": "user", "content": user_input})
        self.conversation_history.append({"role": "assistant", "content": assistant_reply})
        
        # 5. æ¯ 4 æ¡æ¶ˆæ¯å¢é‡æ·»åŠ è®°å¿†
        if len(self.conversation_history) % 4 == 0:
            self._add_memories_incrementally()
        
        return assistant_reply
    
    def force_memorize(self):
        """å¼ºåˆ¶å°†æ‰€æœ‰æœªè®°å¿†çš„å¯¹è¯æ·»åŠ åˆ°è®°å¿†åº“"""
        print("ğŸ”„ å¼ºåˆ¶åŒæ­¥è®°å¿†...")
        self._add_memories_incrementally()
    
    def get_memory_count(self) -> int:
        """è·å–è®°å¿†æ•°é‡"""
        try:
            all_memories = self.mem_cube.text_mem.get_all(user_name=self.user_id)
            if isinstance(all_memories, dict) and 'nodes' in all_memories:
                return len(all_memories.get('nodes', []))
            return len(all_memories)
        except:
            return 0
    
    def show_memories(self, limit: int = 20):
        """æ˜¾ç¤ºæ‰€æœ‰è®°å¿†"""
        try:
            all_memories = self.mem_cube.text_mem.get_all(user_name=self.user_id)
            
            if isinstance(all_memories, dict) and 'nodes' in all_memories:
                nodes = all_memories.get('nodes', [])[:limit]
                print(f"\nğŸ“š å½“å‰å…±æœ‰ {len(nodes)} æ¡è®°å¿†:")
                for i, node in enumerate(nodes, 1):
                    mem_text = node.get('memory', '')[:80]
                    mem_type = node.get('metadata', {}).get('memory_type', 'Unknown')
                    print(f"  [{i}] [{mem_type}] {mem_text}...")
            else:
                print(f"\nğŸ“š å½“å‰å…±æœ‰ {len(all_memories)} æ¡è®°å¿†:")
                for i, mem_item in enumerate(all_memories[:limit], 1):
                    print(f"  [{i}] {mem_item.memory[:80]}...")
        except Exception as e:
            print(f"âŒ è·å–è®°å¿†å¤±è´¥: {e}")
    
    def clear_memories(self):
        """æ¸…ç©ºæ‰€æœ‰è®°å¿†"""
        try:
            self.mem_cube.text_mem.delete_all()
            self.conversation_history = []
            self.memorized_message_count = 0
            print("âœ… è®°å¿†å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")
    
    def close(self):
        """
        å…³é—­ Chatbotï¼Œé‡Šæ”¾èµ„æº
        
        åœ¨ç¨‹åºé€€å‡ºå‰åº”è°ƒç”¨æ­¤æ–¹æ³•ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´ç¨‹åºæ— æ³•æ­£å¸¸é€€å‡º
        """
        try:
            # å…³é—­ tree_text çš„ memory_manager (åŒ…å« reorganizer çº¿ç¨‹)
            if hasattr(self.mem_cube.text_mem, 'memory_manager'):
                self.mem_cube.text_mem.memory_manager.close()
            
            # å…³é—­ Neo4j è¿æ¥
            if hasattr(self.mem_cube.text_mem, 'graph_store'):
                self.mem_cube.text_mem.graph_store.close()
            
            print("âœ… Chatbot èµ„æºå·²é‡Šæ”¾")
        except Exception as e:
            print(f"âš ï¸ å…³é—­ Chatbot èµ„æºæ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼å¯¹è¯"""
    print("=" * 60)
    print("ğŸ¤– MemOS Chatbot - æ‹¥æœ‰é•¿æœŸè®°å¿†çš„èŠå¤©æœºå™¨äºº")
    print("=" * 60)
    
    # åˆå§‹åŒ– Chatbot
    chatbot = MemOSChatbot(user_id="demo_user")
    
    print(f"\nğŸ“Š å½“å‰è®°å¿†æ•°é‡: {chatbot.get_memory_count()}")
    
    # äº¤äº’å¼å¯¹è¯
    print("\nè¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥å‘½ä»¤):")
    print("  - 'quit'/'exit': é€€å‡º")
    print("  - '/memory': æ˜¾ç¤ºå½“å‰è®°å¿†")
    print("  - '/clear': æ¸…ç©ºè®°å¿†")
    print("  - '/save': å¼ºåˆ¶ä¿å­˜è®°å¿†")
    print()
    
    while True:
        try:
            user_input = input("ğŸ‘¤ [You] ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\n\nğŸ‘‹ Goodbye!")
            break
        
        # å¤„ç†ç‰¹æ®Šå‘½ä»¤
        if user_input.lower() in {"quit", "exit", "q"}:
            # é€€å‡ºå‰ä¿å­˜è®°å¿†å¹¶å…³é—­èµ„æº
            chatbot.force_memorize()
            print("ğŸ’¾ è®°å¿†å·²ä¿å­˜")
            chatbot.close()
            print("ğŸ‘‹ Goodbye!")
            break
        
        if user_input == "/memory":
            chatbot.show_memories()
            continue
        
        if user_input == "/clear":
            chatbot.clear_memories()
            continue
        
        if user_input == "/save":
            chatbot.force_memorize()
            print("ğŸ’¾ è®°å¿†å·²ä¿å­˜")
            continue
        
        if not user_input:
            continue
        
        # å¯¹è¯
        response = chatbot.chat(user_input)
        print(f"ğŸ¤– [Assistant] {response}\n")


if __name__ == "__main__":
    main()
