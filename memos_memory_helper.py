"""
MemOS è®°å¿†åŠ©æ‰‹æ¨¡å—
==================

å°è£… MemOS æ ‘å½¢è®°å¿† (tree_text) çš„åˆå§‹åŒ–å’Œæ“ä½œï¼Œä¾› LangChain ä¸­é—´ä»¶ä½¿ç”¨ã€‚

åŠŸèƒ½ï¼š
- åˆå§‹åŒ– MemOS å’Œ MemCube
- è®°å¿†çš„å¢åŠ ã€æ£€ç´¢ã€è·å–
- æ”¯æŒå¢é‡å¼è®°å¿†æ·»åŠ 
"""

import os
import warnings
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", message="Pydantic serializer warnings")
warnings.filterwarnings("ignore", message="`torch_dtype` is deprecated")
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message=".*Neo4j.*")

# è®¾ç½®æ—¥å¿— - éœ€è¦åœ¨å¯¼å…¥ memos ä¹‹å‰å’Œä¹‹åéƒ½è®¾ç½®
logging.getLogger("memos").setLevel(logging.ERROR)
logging.getLogger("neo4j").setLevel(logging.CRITICAL)

from dotenv import load_dotenv
load_dotenv()

# MemOS imports
from memos.configs.mem_os import MemOSConfigFactory
from memos.mem_os.main import MOS
from memos.mem_cube.general import GeneralMemCube
from memos.configs.mem_cube import GeneralMemCubeConfig

# å†æ¬¡è®¾ç½®æ—¥å¿—çº§åˆ« - memos æ¨¡å—å¯¼å…¥åæ‰èƒ½ç”Ÿæ•ˆ
logging.getLogger("memos").setLevel(logging.ERROR)
logging.getLogger("memos.mem_cube.general").setLevel(logging.ERROR)
logging.getLogger("memos.api.config").setLevel(logging.ERROR)


class MemosMemoryHelper:
    """
    MemOS è®°å¿†åŠ©æ‰‹
    
    å°è£…æ ‘å½¢è®°å¿† (tree_text + Neo4j) çš„æ‰€æœ‰æ“ä½œï¼Œ
    ä¸º LangChain ä¸­é—´ä»¶æä¾›ç®€æ´çš„æ¥å£ã€‚
    """
    
    def __init__(
        self,
        user_id: str = "langchain_agent_user",
        top_k: int = 5,
        auto_memorize_interval: int = 4,  # æ¯ N æ¡æ¶ˆæ¯è‡ªåŠ¨æ·»åŠ è®°å¿†
    ):
        """
        åˆå§‹åŒ– MemOS è®°å¿†åŠ©æ‰‹
        
        Args:
            user_id: ç”¨æˆ·IDï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„è®°å¿†
            top_k: è®°å¿†æ£€ç´¢æ—¶è¿”å›çš„æœ€å¤§æ•°é‡
            auto_memorize_interval: è‡ªåŠ¨æ·»åŠ è®°å¿†çš„é—´éš”ï¼ˆæ¶ˆæ¯æ•°ï¼‰
        """
        self.user_id = user_id
        self.top_k = top_k
        self.auto_memorize_interval = auto_memorize_interval
        
        # è·å– API é…ç½®
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        
        if not self.openai_key:
            raise ValueError("âŒ æœªé…ç½® OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        
        # å¯¹è¯å†å²è¿½è¸ª
        self.conversation_history: List[Dict[str, str]] = []
        self.memorized_message_count = 0
        self.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆå§‹åŒ– MemOS
        self._init_memos()
        
        print(f"âœ… MemOS è®°å¿†åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   - ç”¨æˆ·ID: {self.user_id}")
        print(f"   - ä¼šè¯ID: {self.session_id}")
        print(f"   - è®°å¿†æ£€ç´¢ top_k: {self.top_k}")
    
    def _init_memos(self):
        """åˆå§‹åŒ– MemOS è®°å¿†ç³»ç»Ÿ"""
        print("ğŸ“¦ åˆå§‹åŒ– MemOS è®°å¿†ç³»ç»Ÿ...")
        
        # é…ç½® MOS
        mos_config = MemOSConfigFactory(
            config={
                "user_id": "root",
                "chat_model": {
                    "backend": "openai",
                    "config": {
                        "model_name_or_path": "gpt-4o-mini",
                        "temperature": 0.0,
                        "max_tokens": 8192,
                        "api_key": self.openai_key,
                        "api_base": self.openai_base
                    }
                },
                "mem_reader": {
                    "backend": "simple_struct",
                    "config": {
                        "llm": {
                            "backend": "openai",
                            "config": {
                                "model_name_or_path": "gpt-4o-mini",
                                "temperature": 0.0,
                                "max_tokens": 8192,
                                "api_key": self.openai_key,
                                "api_base": self.openai_base
                            }
                        },
                        "embedder": {
                            "backend": "universal_api",
                            "config": {
                                "provider": "openai",
                                "model_name_or_path": "text-embedding-3-small",
                                "api_key": self.openai_key,
                                "base_url": self.openai_base
                            }
                        },
                        "chunker": {
                            "backend": "sentence",
                            "config": {
                                "tokenizer_or_token_counter": "character",
                                "chunk_size": 512,
                                "chunk_overlap": 128,
                                "min_sentences_per_chunk": 1
                            }
                        }
                    }
                },
                "max_turns_window": 20,
                "top_k": self.top_k,
                "enable_textual_memory": True,
                "enable_activation_memory": False,
                "enable_parametric_memory": False,
                "enable_mem_scheduler": False
            }
        )
        
        self.mos = MOS(mos_config.config)
        self.mos.create_user(user_id=self.user_id)
        
        # cube_id å›ºå®š
        self.cube_id = f"{self.user_id}_agent_cube"
        
        # Neo4j é…ç½®
        neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        neo4j_password = os.getenv("NEO4J_PASSWORD", "yourpassword")
        
        print(f"   ğŸ“¡ è¿æ¥ Neo4j: {neo4j_uri}")
        
        # åˆ›å»º MemCube è¿æ¥åˆ° Neo4j (tree_text åç«¯)
        mem_cube_config = GeneralMemCubeConfig(
            user_id=self.user_id,
            cube_id=self.cube_id,
            text_mem={
                "backend": "tree_text",
                "config": {
                    "extractor_llm": {
                        "backend": "openai",
                        "config": {
                            "model_name_or_path": "gpt-4o-mini",
                            "temperature": 0.0,
                            "max_tokens": 8192,
                            "api_key": self.openai_key,
                            "api_base": self.openai_base
                        }
                    },
                    "dispatcher_llm": {
                        "backend": "openai",
                        "config": {
                            "model_name_or_path": "gpt-4o-mini",
                            "temperature": 0.0,
                            "max_tokens": 8192,
                            "api_key": self.openai_key,
                            "api_base": self.openai_base
                        }
                    },
                    "embedder": {
                        "backend": "universal_api",
                        "config": {
                            "provider": "openai",
                            "model_name_or_path": "text-embedding-3-small",
                            "api_key": self.openai_key,
                            "base_url": self.openai_base
                        }
                    },
                    "graph_db": {
                        "backend": "neo4j",
                        "config": {
                            "uri": neo4j_uri,
                            "user": neo4j_user,
                            "password": neo4j_password,
                            "db_name": "memos",
                            "embedding_dimension": 1536
                        }
                    },
                    "reorganize": True
                }
            },
            act_mem={"backend": "uninitialized"},
            para_mem={"backend": "uninitialized"}
        )
        
        self.mem_cube = GeneralMemCube(mem_cube_config)
        self.mos.register_mem_cube(
            mem_cube_name_or_path=self.mem_cube,
            mem_cube_id=self.cube_id,
            user_id=self.user_id
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†å²è®°å¿†
        memory_count = self.get_memory_count()
        if memory_count > 0:
            print(f"   âœ… å‘ç° {memory_count} æ¡å†å²è®°å¿†")
        else:
            print(f"   ğŸ†• æœªå‘ç°å†å²è®°å¿†")
    
    def search_memories(self, query: str) -> List[str]:
        """
        æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³è®°å¿†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            è®°å¿†åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        """
        memories = []
        
        try:
            results = self.mos.search(query=query, user_id=self.user_id)
            
            if results.get("text_mem") and results["text_mem"][0]["memories"]:
                for mem_item in results["text_mem"][0]["memories"][:self.top_k]:
                    memories.append(mem_item.memory)
                    
        except Exception as e:
            print(f"   âš ï¸ æ£€ç´¢è®°å¿†æ—¶å‡ºé”™: {e}")
        
        return memories
    
    def add_conversation(self, user_message: str, assistant_message: str):
        """
        æ·»åŠ ä¸€è½®ç®€å•å¯¹è¯åˆ°å†å²è®°å½•ï¼ˆä¸åŒ…å«å·¥å…·è°ƒç”¨ï¼‰
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            assistant_message: åŠ©æ‰‹å›å¤
        """
        self.conversation_history.append({"role": "user", "content": user_message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ·»åŠ è®°å¿†
        if len(self.conversation_history) % self.auto_memorize_interval == 0:
            self._add_memories_incrementally()
    
    def add_full_conversation(self, messages: List[Dict[str, Any]]):
        """
        æ·»åŠ å®Œæ•´å¯¹è¯åˆ°å†å²è®°å½•ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨
        
        æ”¯æŒçš„æ¶ˆæ¯ç±»å‹ï¼š
        - role: "user" - ç”¨æˆ·æ¶ˆæ¯
        - role: "assistant" - åŠ©æ‰‹æ¶ˆæ¯ï¼ˆå¯åŒ…å« tool_callsï¼‰
        - role: "tool" - å·¥å…·è¿”å›ç»“æœ
        
        Args:
            messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
                [
                    {"role": "user", "content": "æœç´¢ä»Šå¤©çš„æ–°é—»"},
                    {"role": "assistant", "content": "", "tool_calls": [...]},
                    {"role": "tool", "tool_call_id": "xxx", "name": "search", "content": "..."},
                    {"role": "assistant", "content": "æ ¹æ®æœç´¢ç»“æœ..."}
                ]
        """
        for msg in messages:
            self.conversation_history.append(msg)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ·»åŠ è®°å¿†
        if len(self.conversation_history) % self.auto_memorize_interval == 0:
            self._add_memories_incrementally()
    
    def _add_memories_incrementally(self):
        """å¢é‡å¼æ·»åŠ è®°å¿†åˆ° MemOS"""
        current_count = len(self.conversation_history)
        
        if current_count <= self.memorized_message_count:
            return
        
        new_messages = self.conversation_history[self.memorized_message_count:]
        
        if len(new_messages) >= 2:
            try:
                self.mos.add(
                    messages=new_messages,
                    user_id=self.user_id,
                    mem_cube_id=self.cube_id
                )
                self.memorized_message_count = current_count
                print(f"   ğŸ’¾ å·²å°† {len(new_messages)} æ¡æ–°æ¶ˆæ¯æ·»åŠ åˆ°è®°å¿†")
            except Exception as e:
                print(f"   âš ï¸ æ·»åŠ è®°å¿†æ—¶å‡ºé”™: {e}")
    
    def force_memorize(self):
        """å¼ºåˆ¶å°†æ‰€æœ‰æœªè®°å¿†çš„å¯¹è¯æ·»åŠ åˆ°è®°å¿†åº“"""
        print("ğŸ”„ å¼ºåˆ¶åŒæ­¥è®°å¿†...")
        self._add_memories_incrementally()
    
    def get_memory_count(self) -> int:
        """è·å–è®°å¿†æ•°é‡"""
        try:
            all_memories = self.mem_cube.text_mem.get_all(user_name=self.user_id)
            if isinstance(all_memories, dict) and 'nodes' in all_memories:
                return len(all_memories.get('nodes', []))
            return len(all_memories)
        except:
            return 0
    
    def clear_memories(self):
        """æ¸…ç©ºæ‰€æœ‰è®°å¿†"""
        try:
            self.mem_cube.text_mem.delete_all()
            self.conversation_history = []
            self.memorized_message_count = 0
            print("âœ… è®°å¿†å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")
    
    def get_all_memories(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰è®°å¿†
        
        Args:
            limit: è¿”å›çš„æœ€å¤§æ•°é‡
            
        Returns:
            è®°å¿†åˆ—è¡¨
        """
        try:
            all_memories = self.mem_cube.text_mem.get_all(user_name=self.user_id)
            
            if isinstance(all_memories, dict) and 'nodes' in all_memories:
                nodes = all_memories.get('nodes', [])[:limit]
                return [
                    {
                        "memory": node.get('memory', ''),
                        "type": node.get('metadata', {}).get('memory_type', 'Unknown')
                    }
                    for node in nodes
                ]
            else:
                return [
                    {
                        "memory": mem_item.memory,
                        "type": "general"
                    }
                    for mem_item in all_memories[:limit]
                ]
        except Exception as e:
            print(f"âš ï¸ è·å–è®°å¿†å¤±è´¥: {e}")
            return []
    
    def format_memories_for_prompt(self, memories: List[str]) -> str:
        """
        å°†è®°å¿†åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå¯ç”¨äº prompt çš„å­—ç¬¦ä¸²
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        if not memories:
            return "æš‚æ— ç›¸å…³è®°å¿†"
        
        formatted = []
        for i, mem in enumerate(memories, 1):
            formatted.append(f"[{i}] {mem}")
        
        return "\n".join(formatted)
    
    def close(self):
        """
        å…³é—­ MemOS èµ„æºï¼Œé‡Šæ”¾è¿æ¥æ± å’Œåå°çº¿ç¨‹
        
        åœ¨ç¨‹åºé€€å‡ºå‰åº”è°ƒç”¨æ­¤æ–¹æ³•ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´ç¨‹åºæ— æ³•æ­£å¸¸é€€å‡º
        """
        try:
            # å…³é—­ tree_text çš„ memory_manager (åŒ…å« reorganizer çº¿ç¨‹)
            if hasattr(self.mem_cube.text_mem, 'memory_manager'):
                self.mem_cube.text_mem.memory_manager.close()
            
            # å…³é—­ Neo4j è¿æ¥
            if hasattr(self.mem_cube.text_mem, 'graph_store'):
                self.mem_cube.text_mem.graph_store.close()
            
            print("âœ… MemOS èµ„æºå·²é‡Šæ”¾")
        except Exception as e:
            print(f"âš ï¸ å…³é—­ MemOS èµ„æºæ—¶å‡ºé”™: {e}")


# å…¨å±€å•ä¾‹å®ä¾‹ï¼ˆå¯é€‰ä½¿ç”¨ï¼‰
_global_memory_helper: Optional[MemosMemoryHelper] = None


def get_memory_helper(
    user_id: str = "langchain_agent_user",
    top_k: int = 5
) -> MemosMemoryHelper:
    """
    è·å–æˆ–åˆ›å»ºå…¨å±€ MemosMemoryHelper å®ä¾‹
    
    Args:
        user_id: ç”¨æˆ·ID
        top_k: è®°å¿†æ£€ç´¢æ•°é‡
        
    Returns:
        MemosMemoryHelper å®ä¾‹
    """
    global _global_memory_helper
    
    if _global_memory_helper is None:
        _global_memory_helper = MemosMemoryHelper(user_id=user_id, top_k=top_k)
    
    return _global_memory_helper


if __name__ == "__main__":
    # æµ‹è¯•
    helper = MemosMemoryHelper(user_id="test_user")
    
    # æ·»åŠ å¯¹è¯
    helper.add_conversation("æˆ‘å–œæ¬¢è¸¢è¶³çƒ", "å¤ªæ£’äº†ï¼è¿åŠ¨å¯¹èº«ä½“å¾ˆæœ‰å¥½å¤„ã€‚")
    helper.add_conversation("æˆ‘æœ€å–œæ¬¢çš„çƒæ˜Ÿæ˜¯æ¢…è¥¿", "æ¢…è¥¿ç¡®å®æ˜¯ä¸€ä½å‡ºè‰²çš„çƒå‘˜ï¼")
    
    # å¼ºåˆ¶æ·»åŠ è®°å¿†
    helper.force_memorize()
    
    # æœç´¢è®°å¿†
    memories = helper.search_memories("æˆ‘çš„çˆ±å¥½æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("æ£€ç´¢åˆ°çš„è®°å¿†:", memories)
    
    # æ˜¾ç¤ºæ‰€æœ‰è®°å¿†
    all_mems = helper.get_all_memories()
    print("æ‰€æœ‰è®°å¿†:", all_mems)
